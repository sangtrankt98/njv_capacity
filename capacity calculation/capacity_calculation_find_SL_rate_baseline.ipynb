{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "capacity calculation - find SL rate baseline",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSW2sizYP5q2"
      },
      "source": [
        "# Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKCxxCBnvdDY"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime as dt\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import altair as alt\n",
        "import random\n",
        "import scipy.stats as stats\n",
        "import matplotlib.pylab as pylab\n",
        "import matplotlib.gridspec as gridspec\n",
        "pd.options.mode.chained_assignment = None\n",
        "pd.set_option(\"display.max_rows\", 100000,\"display.max_columns\", 100)\n",
        "# !pip install dingsound\n",
        "# import dingsound as d\n",
        "%matplotlib inline"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YX_jSvZ2uhLI",
        "outputId": "f8192ce2-2659-432c-8f7a-ecf04027251a"
      },
      "source": [
        "!git clone 'https://github.com/sangtrankt98/njv_capacity.git'\n",
        "# ! ls\n",
        "# ! rm -rf '/content/njv_capacity'"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'njv_capacity' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7LalKnvvgI7"
      },
      "source": [
        "lm= pd.read_csv('/content/njv_capacity/capacity calculation/lm_index.csv')\n",
        "fm= pd.read_csv('/content/njv_capacity/capacity calculation/fm_index.csv')\n",
        "rts= pd.read_csv('/content/njv_capacity/capacity calculation/rts_index.csv')\n",
        "kpi= pd.read_csv('/content/njv_capacity/capacity calculation/kpi_index.csv')\n",
        "driver= pd.read_csv('/content/njv_capacity/capacity calculation/driver_index.csv')\n",
        "hub= pd.read_csv('/content/njv_capacity/capacity calculation/hub.csv')"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5LGAs601cgr"
      },
      "source": [
        "# +0.05*aggregated['succcess_day3_OTP']\n",
        "aggregated1 = lm.merge(rts,how='left',on= ['cal_date','shipper_group','hub_id'])\n",
        "aggregated2 = aggregated1.merge(fm,how='outer',on=['cal_date','shipper_group','hub_id'])\n",
        "aggregated = aggregated2.merge(hub,how='inner',on=['hub_id'])\n",
        "aggregated.fillna(value = 0,inplace=True)\n",
        "aggregated['cal_date']=pd.to_datetime(aggregated['cal_date'],format='%Y-%m-%d')\n",
        "aggregated['created_month']=pd.to_datetime(aggregated['created_month'],format='%Y-%m')\n",
        "aggregated['area']=aggregated['hub_region'].map({'HN':'Metro','HCM': 'Metro', 'North':'Non-Metro','South': 'Non-Metro'})"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7x2rIvriiIyh"
      },
      "source": [
        "hub_info={\n",
        "'hub_id':\n",
        "[1235,191,1233,1239,963],\n",
        "'hub_name_update':\n",
        "['HCM - Phu Nhuan - SOU - DP','HCM - District 9 A - SOU - DP','HCM - Go Vap A - SOU - DP','HCM - Tan Binh 1 - SOU - DP','HCM - Can Gio 1 - SOU - DP'],\n",
        "'hub_id_update':\n",
        "[236,120,130,823,944]\n",
        "}\n",
        "for i in range(len(hub_info)+1):\n",
        "  aggregated.loc[aggregated['hub_id']==hub_info['hub_id'][i],'hub_name']= hub_info['hub_name_update'][i]\n",
        "  aggregated.loc[aggregated['hub_id']==hub_info['hub_id'][i],'hub_id']= hub_info['hub_id_update'][i]\n",
        "  driver.loc[driver['hub_id']==hub_info['hub_id'][i],'hub_id']= hub_info['hub_id_update'][i]"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCFDrAcplS9c"
      },
      "source": [
        "# Mapping driver number\n",
        "driver_map=driver.merge(hub[['hub_id','hub_region']],how='inner',on='hub_id')\n",
        "driver_map=driver_map.groupby(['cal_date','hub_region']).agg({'rider_no':'sum','parcel_no':'sum'}).reset_index()\n",
        "final_result=pd.DataFrame()\n",
        "sl_rate1=np.around(np.arange(start=0.88, stop=0.98, step=0.01,dtype=np.float64),2).tolist()\n",
        "sl_rate2=np.around(np.arange(start=0.86, stop=0.96, step=0.01,dtype=np.float64),2).tolist()\n",
        "col_list=['first_pu_ontime_rate','pu_success_ontime_rate','first_deli_ontime_rate','deli_succcess_ontime_day1_rate','success_rate']\n",
        "for i,j in zip(sl_rate1,sl_rate2):\n",
        "  kpi.update(kpi.loc[kpi['hub_region'].isin(['HN','HCM'])].replace(to_replace={col:sl_rate1 for col in col_list},value=i),overwrite=True)\n",
        "  kpi.update(kpi.loc[kpi['hub_region'].isin(['North','South'])].replace(to_replace={col:sl_rate2 for col in col_list},value=j),overwrite=True)\n",
        "  calculation = aggregated.merge(kpi,how='left',on=['shipper_group','hub_region'],suffixes=('','_kpi'))\n",
        "  calculation['total_volume']= calculation.apply(lambda row: row.fm_orders + row.lm_orders, axis=1)\n",
        "  # Calculation the percentage of each shipper group\n",
        "  percentage=calculation.groupby(['cal_date','hub_name','shipper_group']).agg({'total_volume':'sum'}).groupby(['cal_date','hub_name']).apply(lambda x: x/x.sum()).reset_index()\n",
        "  percentage.sort_values(by=['cal_date','hub_name'],inplace= True)\n",
        "  percentage.fillna(value = 0,inplace=True)\n",
        "  percentage.columns=['cal_date', 'hub_name', 'shipper_group', 'total_volume_percentage']\n",
        "  # percentage[percentage['total_volume'].isnull()]\n",
        "  calculation=calculation.merge(percentage,how='left',on=['cal_date','shipper_group','hub_name'])\n",
        "  # Calculate service level score\n",
        "  calculation.fillna(value = 0,inplace=True)\n",
        "  calculation['check6']=calculation['first_deli_ontime_rate'].ne(0).astype(int)\n",
        "  calculation['check7']=calculation['deli_succcess_ontime_day1_rate'].ne(0).astype(int)\n",
        "  calculation['check8']=calculation['first_pu_ontime_rate'].ne(0).astype(int)\n",
        "  calculation['check9']=calculation['pu_success_ontime_rate'].ne(0).astype(int)\n",
        "  calculation['check10']=calculation['success_rate'].ne(0).astype(int)\n",
        "  calculation['actual_score']=calculation.apply(lambda x: (x.first_deli_ontime_rate*x.check1 + x.deli_succcess_ontime_day1_rate*x.check2 + x.first_pu_ontime_rate*x.check3 + x.pu_success_ontime_rate*x.check4 + x.success_rate*x.check5)*x.total_volume_percentage , axis=1)\n",
        "  calculation['kpi_score']=calculation.apply(lambda x: (x.first_deli_ontime_rate_kpi*x.check6 + x.deli_succcess_ontime_day1_rate_kpi*x.check7 + x.first_pu_ontime_rate_kpi*x.check8 + x.pu_success_ontime_rate_kpi*x.check9 + x.success_rate_kpi*x.check10)*x.total_volume_percentage , axis=1)\n",
        "\n",
        "  # prepare conditions\n",
        "  pre_final=calculation[['cal_date','shipper_group','hub_region','hub_name','hub_id','fm_orders','lm_orders','total_volume','actual_score','kpi_score']]\n",
        "  pre_final = pre_final[pre_final['total_volume'].isnull()==False]\n",
        "  # Filter 1: active day\n",
        "  filter1 = pre_final.groupby(['hub_name','hub_id']).agg({'cal_date':pd.Series.nunique}).reset_index()\n",
        "  hub_low_time=filter1[filter1['cal_date']<=60]['hub_name'].to_list()\n",
        "  pre_final=pre_final[(~pre_final['hub_name'].isin(hub_low_time))&(pre_final['cal_date']<=dt.datetime(2021,4,30))]\n",
        "  # Filter 2: volume/day\n",
        "  filter2 = pre_final.groupby(['hub_name','hub_id']).agg({'total_volume':'mean'}).reset_index()\n",
        "  hub_low_workload=filter2[filter2['total_volume']<=5]['hub_name'].to_list()\n",
        "  pre_final=pre_final[~pre_final['hub_name'].isin(hub_low_workload)]\n",
        "  #  Filter 3: remove low score\n",
        "  final=pre_final.groupby(['cal_date','hub_region']).agg({'total_volume':'sum','actual_score':'sum','kpi_score':'sum'}).reset_index()\n",
        "  final=final[final['actual_score']>=final['kpi_score']]\n",
        "  # Remove outlier & calulate z-score\n",
        "  final['z_score_total_volume']=final.groupby(['hub_region']).total_volume.transform(lambda x : stats.zscore(x,ddof=1))\n",
        "  final['mean_total_volume']=final.groupby(['hub_region']).total_volume.transform(lambda x : x.mean())\n",
        "  final['std_total_volume']=final.groupby(['hub_region']).total_volume.transform(lambda x : x.std(ddof=1))\n",
        "  final['q1_z']=final.groupby(['hub_region']).z_score_total_volume.transform(lambda x : x.quantile(.25))\n",
        "  final['q3_z']=final.groupby(['hub_region']).z_score_total_volume.transform(lambda x : x.quantile(.75))\n",
        "  final['lower_z']=final.apply(lambda x: 2.5*x.q1_z-1.5*x.q3_z, axis=1)\n",
        "  final['upper_z']=final.apply(lambda x: 2.5*x.q3_z-1.5*x.q1_z, axis=1)\n",
        "  ## Filter\n",
        "  final = final[(final['z_score_total_volume']>=final['lower_z'])&(final['z_score_total_volume']<=final['upper_z'])]\n",
        "  observation = final[['hub_region','cal_date']].groupby(['hub_region']).agg('count').reset_index()\n",
        "  observation.rename(columns={'cal_date': 'observation_num'})\n",
        "  # Calculate confidence interval\n",
        "  hub_region=list(final['hub_region'].unique())\n",
        "  result= pd.DataFrame()\n",
        "  for hub_region in hub_region:\n",
        "    data=stats.bayes_mvs(final[final['hub_region']==hub_region]['total_volume'],alpha= 0.9)\n",
        "    dict =  {\n",
        "    'hub_region':hub_region,\n",
        "    'mean': [data[0][0]],\n",
        "    'lower': [data[0][1][0]],\n",
        "    'upper': [data[0][1][1]],\n",
        "    'sl_rate1': i,\n",
        "    'sl_rate2': j\n",
        "    }\n",
        "    append = pd.DataFrame.from_dict(dict)\n",
        "    result= result.append(append)\n",
        "  result=result.merge(driver_map,how='left',on='hub_region')\n",
        "  result=result.merge(observation,how='left',on='hub_region')\n",
        "  pre_result = result[(result['parcel_no']<=result['upper'])&(result['parcel_no']>=result['lower'])].groupby(['hub_region','sl_rate1','sl_rate2','cal_date']).agg({'mean':'mean','lower':'mean','upper':'mean','rider_no':'mean'}).reset_index()\n",
        "  final_result = final_result.append(pre_result)\n",
        "final_result=final_result.round({'rider_no':0,'mean':-2,'upper':-2,'lower':-2}).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0VPSvifLkf6"
      },
      "source": [
        "result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQFF6iioAm_S"
      },
      "source": [
        "plt.style.use('seaborn-whitegrid')\n",
        "params = {'legend.fontsize': 30,\n",
        "         'axes.labelsize': 20,\n",
        "         'axes.titlesize':30,\n",
        "         'xtick.labelsize':25,\n",
        "         'ytick.labelsize':25,\n",
        "          'axes.titlepad':6.0,\n",
        "          'axes.labelpad':20.0,\n",
        "          'figure.subplot.wspace': 0.08,\n",
        "          'figure.subplot.hspace': 0.4,\n",
        "          }\n",
        "# pylab.rcParams.keys()\n",
        "pylab.rcParams.update(params)\n",
        "fig1, ax1 = plt.subplots(2, 2, figsize=(44, 20),squeeze=False, sharey=True)\n",
        "# fig1.update(wspace=0.05)\n",
        "ax1 = ax1.ravel()\n",
        "for i,j in enumerate(final_result.hub_region.unique().tolist()):\n",
        "  if j in {'HCM','HN'}:\n",
        "    sns.pointplot(x=final_result[final_result['hub_region']==j]['sl_rate1']\n",
        "                    ,y=final_result[final_result['hub_region']==j]['mean']\n",
        "                    ,ci=final_result[final_result['hub_region']==j]['upper']-final_result[final_result['hub_region']==j]['lower']\n",
        "                    ,markers='o',errwidth=3,linestyles='--',color='#E51B23',capthick=0,ax=ax1[i])\n",
        "  else :\n",
        "    sns.pointplot(x=final_result[final_result['hub_region']==j]['sl_rate2']\n",
        "                    ,y=final_result[final_result['hub_region']==j]['mean']\n",
        "                    ,ci=final_result[final_result['hub_region']==j]['upper']-final_result[final_result['hub_region']==j]['lower']\n",
        "                    ,markers='o',errwidth=3,linestyles='--',color='#E51B23',capthick=0,ax=ax1[i])\n",
        "  ax1[i].set_title('Capacity by SL rate ' + str(j),loc='center',pad =20.0)\n",
        "  ax1[i].set_xlabel('service level rate',labelpad =20.0)\n",
        "  ax1[i].set_ylabel('capacity baseline',labelpad =20.0)\n",
        "  ax1[i].set_yticks(ticks=np.arange(20000,140000,20000))\n",
        "  ax1[i].set_ylim(ymin=-2000, ymax=120000)\n",
        "  # sns.despine() remove spines\n",
        "  for p in zip(ax1[i].get_xticks(), final_result[final_result['hub_region']==j]['mean']):\n",
        "      ax1[i].text(p[0],p[1]+6000,format(p[1],'.0f'),color='g',fontsize=19,ha = 'center', va = 'bottom')\n",
        "# plt.savefig('/content/drive/MyDrive/Desktop/img.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKnnK5uUPuha"
      },
      "source": [
        "# Data Final"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5cuQHv5E2j-"
      },
      "source": [
        "final_result.to_csv('/content/drive/MyDrive/Capacity Project/find_sl_rate.csv')\n",
        "final_result"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}